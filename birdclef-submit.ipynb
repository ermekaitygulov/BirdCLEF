{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7136ad4a",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-05-17T21:20:46.941461Z",
     "iopub.status.busy": "2022-05-17T21:20:46.938444Z",
     "iopub.status.idle": "2022-05-17T21:20:52.660550Z",
     "shell.execute_reply": "2022-05-17T21:20:52.659903Z",
     "shell.execute_reply.started": "2022-05-17T21:18:08.042917Z"
    },
    "papermill": {
     "duration": 5.75005,
     "end_time": "2022-05-17T21:20:52.660706",
     "exception": false,
     "start_time": "2022-05-17T21:20:46.910656",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../input/timm-pytorch-image-models/pytorch-image-models-master')\n",
    "sys.path.append('../input/birdclefmodels/')\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import librosa\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import json\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import yaml\n",
    "\n",
    "import torch\n",
    "import torchaudio as ta\n",
    "import timm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import neural_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0687b644",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-17T21:20:52.861729Z",
     "iopub.status.busy": "2022-05-17T21:20:52.861100Z",
     "iopub.status.idle": "2022-05-17T21:20:53.039311Z",
     "shell.execute_reply": "2022-05-17T21:20:53.038793Z",
     "shell.execute_reply.started": "2022-05-17T21:18:21.931227Z"
    },
    "papermill": {
     "duration": 0.359901,
     "end_time": "2022-05-17T21:20:53.039433",
     "exception": false,
     "start_time": "2022-05-17T21:20:52.679532",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_root = '/kaggle/input/birdclef-2022'\n",
    "train_meta = pd.read_csv('../input/birdclef-data-with-wav-durations/train_metadata_extended.csv')\n",
    "ebird_taxonomy = pd.read_csv(os.path.join(data_root, 'eBird_Taxonomy_v2021.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e57ceb71",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-17T21:20:53.114921Z",
     "iopub.status.busy": "2022-05-17T21:20:53.077845Z",
     "iopub.status.idle": "2022-05-17T21:20:53.152514Z",
     "shell.execute_reply": "2022-05-17T21:20:53.152100Z",
     "shell.execute_reply.started": "2022-05-17T21:18:23.319029Z"
    },
    "papermill": {
     "duration": 0.095082,
     "end_time": "2022-05-17T21:20:53.152625",
     "exception": false,
     "start_time": "2022-05-17T21:20:53.057543",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_meta.loc[:, 'secondary_labels'] = train_meta.secondary_labels.apply(eval)\n",
    "train_meta['target_raw'] = train_meta.secondary_labels + train_meta.primary_label.apply(lambda x: [x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c32be283",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-17T21:20:53.641761Z",
     "iopub.status.busy": "2022-05-17T21:20:53.621385Z",
     "iopub.status.idle": "2022-05-17T21:20:53.941275Z",
     "shell.execute_reply": "2022-05-17T21:20:53.941753Z",
     "shell.execute_reply.started": "2022-05-17T21:18:23.677966Z"
    },
    "papermill": {
     "duration": 0.771539,
     "end_time": "2022-05-17T21:20:53.941905",
     "exception": false,
     "start_time": "2022-05-17T21:20:53.170366",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_species = sorted(set(train_meta.target_raw.sum()))\n",
    "species2id = {s: i for i, s in enumerate(all_species)}\n",
    "id2species = {i: s for i, s in enumerate(all_species)}\n",
    "\n",
    "train_meta['target'] = train_meta.target_raw.apply(lambda species: [int(s in species) for s in all_species])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "246e639b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-17T21:20:53.982870Z",
     "iopub.status.busy": "2022-05-17T21:20:53.982178Z",
     "iopub.status.idle": "2022-05-17T21:20:53.984779Z",
     "shell.execute_reply": "2022-05-17T21:20:53.984300Z",
     "shell.execute_reply.started": "2022-05-17T21:18:25.534002Z"
    },
    "papermill": {
     "duration": 0.024975,
     "end_time": "2022-05-17T21:20:53.984893",
     "exception": false,
     "start_time": "2022-05-17T21:20:53.959918",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_wav(fname, offset, duration):\n",
    "#     fname = 'afrsil1/XC125458.ogg'\n",
    "    fpath = os.path.join(data_root, 'train_audio', fname)\n",
    "    wav, sr = librosa.load(fpath, sr=None, duration=duration)\n",
    "    assert sr <= 32000, sr\n",
    "    return wav, sr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f7f0c3",
   "metadata": {
    "papermill": {
     "duration": 0.017356,
     "end_time": "2022-05-17T21:20:54.019907",
     "exception": false,
     "start_time": "2022-05-17T21:20:54.002551",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Torch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acf0cabf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-17T21:20:54.060245Z",
     "iopub.status.busy": "2022-05-17T21:20:54.058747Z",
     "iopub.status.idle": "2022-05-17T21:20:54.060882Z",
     "shell.execute_reply": "2022-05-17T21:20:54.061315Z",
     "shell.execute_reply.started": "2022-05-17T21:18:27.015809Z"
    },
    "papermill": {
     "duration": 0.023656,
     "end_time": "2022-05-17T21:20:54.061435",
     "exception": false,
     "start_time": "2022-05-17T21:20:54.037779",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "TEST_SIZE = 5 \n",
    "CONFIG = {\n",
    "    'crop_len': 30,\n",
    "    'sample_rate': 32000,    \n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7711e847",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T09:17:01.036366Z",
     "iopub.status.busy": "2022-04-21T09:17:01.036048Z",
     "iopub.status.idle": "2022-04-21T09:17:01.05202Z",
     "shell.execute_reply": "2022-04-21T09:17:01.050779Z",
     "shell.execute_reply.started": "2022-04-21T09:17:01.036334Z"
    },
    "papermill": {
     "duration": 0.017645,
     "end_time": "2022-05-17T21:20:54.096628",
     "exception": false,
     "start_time": "2022-05-17T21:20:54.078983",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db91cb71",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-17T21:20:54.140792Z",
     "iopub.status.busy": "2022-05-17T21:20:54.140232Z",
     "iopub.status.idle": "2022-05-17T21:20:54.143669Z",
     "shell.execute_reply": "2022-05-17T21:20:54.143238Z",
     "shell.execute_reply.started": "2022-05-17T21:18:27.964681Z"
    },
    "papermill": {
     "duration": 0.029569,
     "end_time": "2022-05-17T21:20:54.143778",
     "exception": false,
     "start_time": "2022-05-17T21:20:54.114209",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from torch.distributions import Beta\n",
    "# from torch import nn\n",
    "\n",
    "\n",
    "# class Net(torch.nn.Module):\n",
    "#     def __init__(self, backbone_path=None):\n",
    "#         super().__init__()\n",
    "#         self.audio2image = self._init_audio2image()\n",
    "#         self.backbone = self._init_backbone()\n",
    "#         self.load_backbone(backbone_path)\n",
    "#         self.head = self._init_head(self.backbone.feature_info[-1]['num_chs'])      \n",
    "#         self.loss = torch.nn.BCEWithLogitsLoss()\n",
    "#         self.mixup = Mixup()\n",
    "        \n",
    "#     def forward(self, wav_tensor, y=None):\n",
    "#         # wav_tensor: b, t\n",
    "#         if self.training:\n",
    "#             wav_tensor = self.batch_crop(wav_tensor) # b, t\n",
    "            \n",
    "#         spectrogram = self.audio2image(wav_tensor) # b, m, t\n",
    "#         spectrogram = spectrogram.permute(0, 2, 1) # b, t, m\n",
    "#         spectrogram = spectrogram[:, None, :, :] # b, c, t, m\n",
    "        \n",
    "#         if self.training:\n",
    "#             spectrogram = spectrogram.permute(0, 2, 1, 3) # b, t, c, m\n",
    "#             spectrogram = self.batch_uncrop(spectrogram)\n",
    "            \n",
    "#             spectrogram, y = self.mixup(spectrogram, y)\n",
    "            \n",
    "#             spectrogram = self.batch_crop(spectrogram)\n",
    "#             spectrogram = spectrogram.permute(0, 2, 1, 3) # b, c, t, m\n",
    "                \n",
    "#         x = self.backbone(spectrogram) # b, c, t, m\n",
    "#         if self.training:\n",
    "#             x = x.permute(0, 2, 1, 3) # b, t, c, m\n",
    "#             x = self.batch_uncrop(x)\n",
    "#             x = x.permute(0, 2, 1, 3) # b, c, t, m\n",
    "        \n",
    "#         # average mel axis\n",
    "#         x = torch.mean(x, axis=-1)\n",
    "                \n",
    "#         logits = self.head(x) # b, n_out\n",
    "        \n",
    "#         if y is not None:\n",
    "#             loss = self.loss(logits, y)\n",
    "#         else:\n",
    "#             loss = None\n",
    "\n",
    "#         return {'loss': loss, 'logits': logits.sigmoid()}\n",
    "    \n",
    "#     def batch_crop(self, tensor):\n",
    "#         factor = int(CONFIG['crop_len'] // TEST_SIZE)\n",
    "#         b, t = tensor.shape[:2]\n",
    "#         tensor = tensor.reshape(b * factor, t // factor, *tensor.shape[2:])\n",
    "#         return tensor\n",
    "    \n",
    "#     def batch_uncrop(self, tensor):\n",
    "#         factor = int(CONFIG['crop_len'] // TEST_SIZE)\n",
    "#         b, t = tensor.shape[:2]\n",
    "#         tensor = tensor.reshape(b // factor, t * factor, *tensor.shape[2:])\n",
    "#         return tensor\n",
    "    \n",
    "#     @staticmethod\n",
    "#     def _init_audio2image():\n",
    "#         mel = ta.transforms.MelSpectrogram(\n",
    "#             sample_rate=32000,\n",
    "#             n_fft=2048,\n",
    "#             win_length=2048,\n",
    "#             hop_length=512,\n",
    "#             f_min=16,\n",
    "#             f_max=16386,\n",
    "#             pad=0,\n",
    "#             n_mels=256,\n",
    "#             power=2,\n",
    "#             normalized=False,\n",
    "#         )\n",
    "#         db_scale = ta.transforms.AmplitudeToDB(top_db=80.0)\n",
    "#         audio2image = torch.nn.Sequential(mel, db_scale)\n",
    "#         return audio2image\n",
    "    \n",
    "#     @staticmethod\n",
    "#     def _init_backbone():\n",
    "#         backbone = \"resnet18\"\n",
    "#         pretrained = False\n",
    "#         pretrained_weights = None\n",
    "#         train = True\n",
    "#         val = False\n",
    "#         in_chans = 1\n",
    "\n",
    "#         backbone = timm.create_model(\n",
    "#             backbone,\n",
    "#             pretrained=pretrained,\n",
    "#             num_classes=0,\n",
    "#             global_pool=\"\",\n",
    "#             in_chans=in_chans,\n",
    "#         )\n",
    "#         return backbone\n",
    "    \n",
    "#     @staticmethod\n",
    "#     def _init_head(num_chs):\n",
    "#         head = Attention(num_chs, len(all_species), activation='linear')\n",
    "#         return head\n",
    "    \n",
    "#     def load_backbone(self, weights_path=None):\n",
    "#         if weights_path:\n",
    "#             state_dict=torch.load(weights_path)\n",
    "#             conv1_weight = state_dict['conv1.weight']\n",
    "#             state_dict['conv1.weight'] = conv1_weight.sum(dim=1, keepdim=True)\n",
    "#             state_dict.pop('fc.bias')\n",
    "#             state_dict.pop('fc.weight')\n",
    "#             self.backbone.load_state_dict(state_dict)\n",
    "\n",
    "\n",
    "# class Mixup(nn.Module):\n",
    "#     def __init__(self, mix_beta=1):\n",
    "#         super(Mixup, self).__init__()\n",
    "#         self.beta_distribution = Beta(mix_beta, mix_beta)\n",
    "\n",
    "#     def forward(self, X, Y, sample_weight=None):\n",
    "\n",
    "#         bs = X.shape[0]\n",
    "#         n_dims = len(X.shape)\n",
    "#         perm = torch.randperm(bs)\n",
    "#         mixup_weight = self.beta_distribution.rsample(torch.Size((bs,))).to(X.device)\n",
    "\n",
    "#         if n_dims == 2:\n",
    "#             X = mixup_weight.view(-1, 1) * X + (1 - mixup_weight.view(-1, 1)) * X[perm]\n",
    "#         elif n_dims == 3:\n",
    "#             X = mixup_weight.view(-1, 1, 1) * X + (1 - mixup_weight.view(-1, 1, 1)) * X[perm]\n",
    "#         else:\n",
    "#             X = mixup_weight.view(-1, 1, 1, 1) * X + (1 - mixup_weight.view(-1, 1, 1, 1)) * X[perm]\n",
    "\n",
    "#         Y = mixup_weight.view(-1, 1) * Y + (1 - mixup_weight.view(-1, 1)) * Y[perm]\n",
    "\n",
    "#         if sample_weight is None:\n",
    "#             return X, Y\n",
    "#         else:\n",
    "#             sample_weight = mixup_weight.view(-1) * sample_weight + (1 - mixup_weight.view(-1)) * sample_weight[perm]\n",
    "#             return X, Y, sample_weight\n",
    "\n",
    "        \n",
    "# class Attention(nn.Module):\n",
    "#     def __init__(self, in_channels, out_channels, activation='linear'):\n",
    "#         super().__init__()\n",
    "#         self.activation = activation\n",
    "#         self.attn = nn.Conv1d(in_channels, out_channels, kernel_size=1)\n",
    "#         self.cla = nn.Conv1d(in_channels, out_channels, kernel_size=1)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         # x: b, c, t\n",
    "#         attn = torch.softmax(torch.tanh(self.attn(x)), dim=-1) # b, c, t\n",
    "#         x = self.cla(x) # b, c, t\n",
    "#         x = torch.sum(x * attn, dim=-1) #b, c\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c28478",
   "metadata": {
    "papermill": {
     "duration": 0.017456,
     "end_time": "2022-05-17T21:20:54.179043",
     "exception": false,
     "start_time": "2022-05-17T21:20:54.161587",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Model load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf965238",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-17T21:20:54.285811Z",
     "iopub.status.busy": "2022-05-17T21:20:54.285230Z",
     "iopub.status.idle": "2022-05-17T21:20:58.027546Z",
     "shell.execute_reply": "2022-05-17T21:20:58.027023Z",
     "shell.execute_reply.started": "2022-05-17T21:18:29.190740Z"
    },
    "papermill": {
     "duration": 3.831103,
     "end_time": "2022-05-17T21:20:58.027678",
     "exception": false,
     "start_time": "2022-05-17T21:20:54.196575",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from neural_network import NN_CATALOG\n",
    "\n",
    "config_path = '../input/birdclefsubmit/baseline_config.yaml'\n",
    "model_path = '../input/birdclefsubmit/final-model.pt'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "with open(config_path) as fin:\n",
    "    config = yaml.safe_load(fin)\n",
    "\n",
    "model_config = config['model']\n",
    "if 'backbone_config' in model_config['params']:\n",
    "    model_config['params']['backbone_config']['pretrained'] = False\n",
    "else:\n",
    "    model_config['params']['backbone_config'] = {'pretrained': False}\n",
    "data_config = config['data']\n",
    "model_class = NN_CATALOG[model_config['name']]\n",
    "\n",
    "model = model_class(len(all_species), int(data_config['crop_len'] // data_config['test_wav_len']),\n",
    "                    **model_config['params'])\n",
    "model.to(device)\n",
    "\n",
    "state_dict = torch.load(model_path, map_location=device)\n",
    "model.load_state_dict(state_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a46d33f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-17T21:20:58.071822Z",
     "iopub.status.busy": "2022-05-17T21:20:58.070259Z",
     "iopub.status.idle": "2022-05-17T21:20:58.072471Z",
     "shell.execute_reply": "2022-05-17T21:20:58.072879Z",
     "shell.execute_reply.started": "2022-05-17T21:18:34.284236Z"
    },
    "papermill": {
     "duration": 0.024914,
     "end_time": "2022-05-17T21:20:58.073022",
     "exception": false,
     "start_time": "2022-05-17T21:20:58.048108",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# state_dict = torch.load(model_path, map_location=device)\n",
    "# model = Net(len(all_species), int(CONFIG['crop_len'] // TEST_SIZE),\n",
    "#             backbone_config=dict(model_name='resnet18', pretrained=False))\n",
    "# model.load_state_dict(state_dict)\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff34a44",
   "metadata": {
    "papermill": {
     "duration": 0.017794,
     "end_time": "2022-05-17T21:20:58.108721",
     "exception": false,
     "start_time": "2022-05-17T21:20:58.090927",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1479d7c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-17T21:20:58.147621Z",
     "iopub.status.busy": "2022-05-17T21:20:58.146845Z",
     "iopub.status.idle": "2022-05-17T21:20:58.154889Z",
     "shell.execute_reply": "2022-05-17T21:20:58.154476Z",
     "shell.execute_reply.started": "2022-05-17T21:18:35.891279Z"
    },
    "papermill": {
     "duration": 0.028318,
     "end_time": "2022-05-17T21:20:58.154990",
     "exception": false,
     "start_time": "2022-05-17T21:20:58.126672",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(data_root, 'scored_birds.json')) as fin:\n",
    "    test_birds = json.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2dd9b174",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-17T21:20:58.200822Z",
     "iopub.status.busy": "2022-05-17T21:20:58.198754Z",
     "iopub.status.idle": "2022-05-17T21:20:58.203075Z",
     "shell.execute_reply": "2022-05-17T21:20:58.202654Z",
     "shell.execute_reply.started": "2022-05-17T21:18:37.382088Z"
    },
    "papermill": {
     "duration": 0.029775,
     "end_time": "2022-05-17T21:20:58.203185",
     "exception": false,
     "start_time": "2022-05-17T21:20:58.173410",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Compose:\n",
    "    def __init__(self, transforms: list):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, y: np.ndarray, sr):\n",
    "        for trns in self.transforms:\n",
    "            y = trns(y, sr)\n",
    "        return y\n",
    "    \n",
    "    \n",
    "class AudioTransform:\n",
    "    def __init__(self, always_apply=False, p=0.5):\n",
    "        self.always_apply = always_apply\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, y: np.ndarray, sr):\n",
    "        if self.always_apply:\n",
    "            return self.apply(y, sr=sr)\n",
    "        else:\n",
    "            if np.random.rand() < self.p:\n",
    "                return self.apply(y, sr=sr)\n",
    "            else:\n",
    "                return y\n",
    "\n",
    "    def apply(self, y: np.ndarray, **params):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "        \n",
    "class Normalize(AudioTransform):\n",
    "    def __init__(self, always_apply=False, p=1):\n",
    "        super().__init__(always_apply, p)\n",
    "\n",
    "    def apply(self, y: np.ndarray, **params):\n",
    "        max_vol = np.abs(y).max()\n",
    "        y_vol = y / max_vol\n",
    "        assert not np.isnan(y_vol).any(), f'{max_vol}'\n",
    "        return y_vol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bea17686",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-17T21:20:58.243128Z",
     "iopub.status.busy": "2022-05-17T21:20:58.241240Z",
     "iopub.status.idle": "2022-05-17T21:20:58.251487Z",
     "shell.execute_reply": "2022-05-17T21:20:58.251055Z",
     "shell.execute_reply.started": "2022-05-17T21:18:38.280000Z"
    },
    "papermill": {
     "duration": 0.030495,
     "end_time": "2022-05-17T21:20:58.251588",
     "exception": false,
     "start_time": "2022-05-17T21:20:58.221093",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, test_folder, augmentations=None):\n",
    "        super().__init__()\n",
    "        self.test_folder = test_folder\n",
    "        self.fnames = [f for f in os.listdir(test_folder) if f.endswith('.ogg')]\n",
    "        self.augmentations = augmentations\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.fnames)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        fpath = os.path.join(self.test_folder, self.fnames[idx])\n",
    "        wav, sr = load_wav(fpath, 0, None)\n",
    "        if self.augmentations:\n",
    "            wav = self.augmentations(wav, None)\n",
    "        wav = torch.tensor(wav)\n",
    "        assert (13 * 5 * sr) > len(wav) \n",
    "        wav = wav[:len(wav) // 12 * 12].reshape((12, len(wav) // 12))\n",
    "        return wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3041cce6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-17T21:20:58.291952Z",
     "iopub.status.busy": "2022-05-17T21:20:58.291433Z",
     "iopub.status.idle": "2022-05-17T21:20:58.296932Z",
     "shell.execute_reply": "2022-05-17T21:20:58.296518Z",
     "shell.execute_reply.started": "2022-05-17T21:18:39.136098Z"
    },
    "papermill": {
     "duration": 0.027578,
     "end_time": "2022-05-17T21:20:58.297093",
     "exception": false,
     "start_time": "2022-05-17T21:20:58.269515",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dataset = TestDataset(\n",
    "    os.path.join(data_root, 'test_soundscapes'),\n",
    "    Compose([Normalize(p=1)])\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    drop_last=False,   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36014f69",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-17T21:20:58.336270Z",
     "iopub.status.busy": "2022-05-17T21:20:58.335485Z",
     "iopub.status.idle": "2022-05-17T21:20:58.341028Z",
     "shell.execute_reply": "2022-05-17T21:20:58.340564Z",
     "shell.execute_reply.started": "2022-05-17T21:18:39.773173Z"
    },
    "papermill": {
     "duration": 0.025771,
     "end_time": "2022-05-17T21:20:58.341144",
     "exception": false,
     "start_time": "2022-05-17T21:20:58.315373",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_topk(pred_proba, max_birds=5):\n",
    "    pred_proba = pred_proba[:, [species2id[b] for b in test_birds]]\n",
    "    mean_proba = pred_proba.mean(axis=0)\n",
    "    topk_birds = [i for i, _ in sorted(enumerate(mean_proba),\n",
    "                                       key=lambda x: x[1],\n",
    "                                       reverse=True)][:max_birds]\n",
    "    return topk_birds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5558b2b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-17T21:20:58.383152Z",
     "iopub.status.busy": "2022-05-17T21:20:58.382407Z",
     "iopub.status.idle": "2022-05-17T21:20:58.384756Z",
     "shell.execute_reply": "2022-05-17T21:20:58.384347Z",
     "shell.execute_reply.started": "2022-05-17T21:18:41.290325Z"
    },
    "papermill": {
     "duration": 0.025557,
     "end_time": "2022-05-17T21:20:58.384853",
     "exception": false,
     "start_time": "2022-05-17T21:20:58.359296",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "treshold_dict = {'akiapo': 0.1,\n",
    " 'aniani': 0.05,\n",
    " 'apapan': 0.05,\n",
    " 'barpet': 0.05,\n",
    " 'crehon': 0.05,\n",
    " 'elepai': 0.05,\n",
    " 'ercfra': 0.01,\n",
    " 'hawama': 0.05,\n",
    " 'hawcre': 0.05,\n",
    " 'hawgoo': 0.05,\n",
    " 'hawhaw': 0.01,\n",
    " 'hawpet1': 0.05,\n",
    " 'houfin': 0.1,\n",
    " 'iiwi': 0.2,\n",
    " 'jabwar': 0.05,\n",
    " 'maupar': 0.05,\n",
    " 'omao': 0.05,\n",
    " 'puaioh': 0.05,\n",
    " 'skylar': 0.1,\n",
    " 'warwhe1': 0.05,\n",
    " 'yefcan': 0.15}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "76c71f8c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-17T21:20:58.429888Z",
     "iopub.status.busy": "2022-05-17T21:20:58.429327Z",
     "iopub.status.idle": "2022-05-17T21:21:05.126965Z",
     "shell.execute_reply": "2022-05-17T21:21:05.127413Z",
     "shell.execute_reply.started": "2022-05-17T21:18:48.618996Z"
    },
    "papermill": {
     "duration": 6.724786,
     "end_time": "2022-05-17T21:21:05.127574",
     "exception": false,
     "start_time": "2022-05-17T21:20:58.402788",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:06,  6.64s/it]\n"
     ]
    }
   ],
   "source": [
    "pred_list = []\n",
    "treshold = 0.1\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, batch in tqdm(enumerate(test_dataloader)):\n",
    "        batch_size, part_count, part_size = batch.shape\n",
    "        batch = batch.reshape(batch_size * part_count, part_size)\n",
    "        pred = model(batch.to(device))['logits']\n",
    "        pred = pred.cpu().numpy()\n",
    "#         topk_birds = find_topk(pred_proba, max_birds=10)\n",
    "        \n",
    "        for j, chunk_pred in enumerate(pred):\n",
    "            inbatch_number = j // part_count\n",
    "            chunk_number = j % part_count + 1\n",
    "            f_idx = i * batch_size + inbatch_number\n",
    "            fname = test_dataset.fnames[f_idx]\n",
    "            prefix = fname.split('.')[0]\n",
    "            sufix = f'{5 * chunk_number}'\n",
    "            \n",
    "            pred_list.extend([{\n",
    "                'row_id': '_'.join([prefix, b, sufix]),\n",
    "                'target': chunk_pred[species2id[b]] > treshold_dict[b] # if species2id[b] in topk_birds else False\n",
    "            } for b in test_birds])\n",
    "pred_pd = pd.DataFrame(pred_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f701760",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-17T21:21:05.172118Z",
     "iopub.status.busy": "2022-05-17T21:21:05.171538Z",
     "iopub.status.idle": "2022-05-17T21:21:05.188296Z",
     "shell.execute_reply": "2022-05-17T21:21:05.187758Z",
     "shell.execute_reply.started": "2022-05-17T21:18:56.822878Z"
    },
    "papermill": {
     "duration": 0.041132,
     "end_time": "2022-05-17T21:21:05.188411",
     "exception": false,
     "start_time": "2022-05-17T21:21:05.147279",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>soundscape_453028782_akiapo_5</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>soundscape_453028782_aniani_5</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>soundscape_453028782_apapan_5</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>soundscape_453028782_barpet_5</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>soundscape_453028782_crehon_5</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>soundscape_453028782_omao_60</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>soundscape_453028782_puaioh_60</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>soundscape_453028782_skylar_60</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>soundscape_453028782_warwhe1_60</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>soundscape_453028782_yefcan_60</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>252 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              row_id  target\n",
       "0      soundscape_453028782_akiapo_5   False\n",
       "1      soundscape_453028782_aniani_5    True\n",
       "2      soundscape_453028782_apapan_5    True\n",
       "3      soundscape_453028782_barpet_5   False\n",
       "4      soundscape_453028782_crehon_5   False\n",
       "..                               ...     ...\n",
       "247     soundscape_453028782_omao_60    True\n",
       "248   soundscape_453028782_puaioh_60   False\n",
       "249   soundscape_453028782_skylar_60   False\n",
       "250  soundscape_453028782_warwhe1_60    True\n",
       "251   soundscape_453028782_yefcan_60   False\n",
       "\n",
       "[252 rows x 2 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_pd.to_csv(\"submission.csv\", index=False)\n",
    "pred_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4a6165",
   "metadata": {
    "papermill": {
     "duration": 0.019911,
     "end_time": "2022-05-17T21:21:05.228195",
     "exception": false,
     "start_time": "2022-05-17T21:21:05.208284",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 29.276868,
   "end_time": "2022-05-17T21:21:07.703718",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-05-17T21:20:38.426850",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
