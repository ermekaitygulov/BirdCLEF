{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\nimport sys\nsys.path.append('../input/timm-pytorch-image-models/pytorch-image-models-master')\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom matplotlib import pyplot as plt\nimport json\nimport random\n\nimport librosa\n\nimport torch\nimport torchaudio as ta\nimport timm\n\nfrom tqdm import tqdm\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-01T16:01:15.222885Z","iopub.execute_input":"2022-05-01T16:01:15.223439Z","iopub.status.idle":"2022-05-01T16:01:15.229668Z","shell.execute_reply.started":"2022-05-01T16:01:15.223381Z","shell.execute_reply":"2022-05-01T16:01:15.228746Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"data_root = '/kaggle/input/birdclef-2022'\ntrain_meta = pd.read_csv('../input/birdclef-data-with-wav-durations/train_metadata_extended.csv')\nebird_taxonomy = pd.read_csv(os.path.join(data_root, 'eBird_Taxonomy_v2021.csv'))","metadata":{"execution":{"iopub.status.busy":"2022-05-01T16:01:17.821194Z","iopub.execute_input":"2022-05-01T16:01:17.821753Z","iopub.status.idle":"2022-05-01T16:01:17.961494Z","shell.execute_reply.started":"2022-05-01T16:01:17.821697Z","shell.execute_reply":"2022-05-01T16:01:17.960786Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"train_meta.loc[:, 'secondary_labels'] = train_meta.secondary_labels.apply(eval)\ntrain_meta['target_raw'] = train_meta.secondary_labels + train_meta.primary_label.apply(lambda x: [x])","metadata":{"execution":{"iopub.status.busy":"2022-05-01T16:01:22.001777Z","iopub.execute_input":"2022-05-01T16:01:22.002180Z","iopub.status.idle":"2022-05-01T16:01:22.361066Z","shell.execute_reply.started":"2022-05-01T16:01:22.002137Z","shell.execute_reply":"2022-05-01T16:01:22.360078Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"all_species = sorted(set(train_meta.target_raw.sum()))\nspecies2id = {s: i for i, s in enumerate(all_species)}\nid2species = {i: s for i, s in enumerate(all_species)}\n\ntrain_meta['target'] = train_meta.target_raw.apply(lambda species: [int(s in species) for s in all_species])","metadata":{"execution":{"iopub.status.busy":"2022-05-01T16:01:28.193322Z","iopub.execute_input":"2022-05-01T16:01:28.194013Z","iopub.status.idle":"2022-05-01T16:01:29.097842Z","shell.execute_reply.started":"2022-05-01T16:01:28.193967Z","shell.execute_reply":"2022-05-01T16:01:29.097002Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def load_wav(fname, offset, duration):\n#     fname = 'afrsil1/XC125458.ogg'\n    fpath = os.path.join(data_root, 'train_audio', fname)\n    wav, sr = librosa.load(fpath, sr=None, duration=duration)\n    assert sr <= 32000, sr\n    return wav, sr","metadata":{"execution":{"iopub.status.busy":"2022-05-01T16:01:33.820531Z","iopub.execute_input":"2022-05-01T16:01:33.821184Z","iopub.status.idle":"2022-05-01T16:01:33.827145Z","shell.execute_reply.started":"2022-05-01T16:01:33.821140Z","shell.execute_reply":"2022-05-01T16:01:33.826171Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"### Torch Dataset","metadata":{}},{"cell_type":"code","source":"TEST_SIZE = 5 \nCONFIG = {\n    'crop_len': 30,\n    'sample_rate': 32000,    \n}\n","metadata":{"execution":{"iopub.status.busy":"2022-05-01T16:01:39.594867Z","iopub.execute_input":"2022-05-01T16:01:39.595246Z","iopub.status.idle":"2022-05-01T16:01:39.601892Z","shell.execute_reply.started":"2022-05-01T16:01:39.595212Z","shell.execute_reply":"2022-05-01T16:01:39.600893Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\n\nclass BirdDataset(Dataset):\n    def __init__(self, df):\n        super().__init__()\n        self.df = df\n        \n    def __getitem__(self, idx):\n        duration = CONFIG['crop_len']\n        sample_rate = CONFIG['sample_rate']\n        \n        fname = self.df.iloc[idx]['filename']\n        wav_len = train_meta.iloc[0]['duration']\n        \n        max_offset = max(0, wav_len - duration)\n        random_offset = random.randint(0, max_offset)\n                \n        wav, sr = load_wav(fname, random_offset, duration)\n        to_pad = duration * sample_rate - wav.shape[0]\n        if to_pad > 0:\n            wav = np.pad(wav, (0, to_pad))\n            \n        target = self.df.iloc[idx]['target']\n        \n        # TODO: add weighting\n            \n        wav = torch.tensor(wav)\n        target = torch.tensor(target, dtype=float)\n        return {\n            'wav': wav,\n            'target': target,\n        }\n\n    def __len__(self):\n        return len(self.df)","metadata":{"execution":{"iopub.status.busy":"2022-05-01T16:01:39.933397Z","iopub.execute_input":"2022-05-01T16:01:39.934449Z","iopub.status.idle":"2022-05-01T16:01:39.947044Z","shell.execute_reply.started":"2022-05-01T16:01:39.934382Z","shell.execute_reply":"2022-05-01T16:01:39.945781Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"### Model","metadata":{"execution":{"iopub.status.busy":"2022-04-21T09:17:01.036048Z","iopub.execute_input":"2022-04-21T09:17:01.036366Z","iopub.status.idle":"2022-04-21T09:17:01.05202Z","shell.execute_reply.started":"2022-04-21T09:17:01.036334Z","shell.execute_reply":"2022-04-21T09:17:01.050779Z"}}},{"cell_type":"code","source":"from torch.distributions import Beta\n\n\nclass Mixup(torch.nn.Module):\n    def __init__(self, mix_beta=1):\n\n        super(Mixup, self).__init__()\n        self.beta_distribution = Beta(mix_beta, mix_beta)\n\n    def forward(self, X, Y, weight=None):\n\n        bs = X.shape[0]\n        n_dims = len(X.shape)\n        perm = torch.randperm(bs)\n        coeffs = self.beta_distribution.rsample(torch.Size((bs,))).to(X.device)\n\n        if n_dims == 2:\n            X = coeffs.view(-1, 1) * X + (1 - coeffs.view(-1, 1)) * X[perm]\n        elif n_dims == 3:\n            X = coeffs.view(-1, 1, 1) * X + (1 - coeffs.view(-1, 1, 1)) * X[perm]\n        else:\n            X = coeffs.view(-1, 1, 1, 1) * X + (1 - coeffs.view(-1, 1, 1, 1)) * X[perm]\n\n        Y = coeffs.view(-1, 1) * Y + (1 - coeffs.view(-1, 1)) * Y[perm]\n\n        if weight is None:\n            return X, Y\n        else:\n            weight = coeffs.view(-1) * weight + (1 - coeffs.view(-1)) * weight[perm]\n            return X, Y, weight","metadata":{"execution":{"iopub.status.busy":"2022-05-01T16:01:42.174210Z","iopub.execute_input":"2022-05-01T16:01:42.174630Z","iopub.status.idle":"2022-05-01T16:01:42.191663Z","shell.execute_reply.started":"2022-05-01T16:01:42.174580Z","shell.execute_reply":"2022-05-01T16:01:42.190025Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"class Net(torch.nn.Module):\n    def __init__(self, backbone_path=None):\n        super().__init__()\n        self.audio2image = self._init_audio2image()\n        self.backbone = self._init_backbone()\n        self.load_backbone(backbone_path)\n        self.head = self._init_head(self.backbone.feature_info[-1]['num_chs'])      \n        self.loss = torch.nn.BCEWithLogitsLoss()\n        self.mixup = Mixup()\n        \n    def forward(self, wav_tensor, y=None):\n        if self.training:\n            wav_tensor = self.batch_crop(wav_tensor)\n            \n        spectrogram = self.audio2image(wav_tensor)\n        spectrogram = spectrogram.permute(0, 2, 1)\n        spectrogram = spectrogram[:, None, :, :]\n        \n        if self.training:\n            spectrogram = spectrogram.permute(0, 2, 1, 3)\n            spectrogram = self.batch_uncrop(spectrogram)\n            \n            spectrogram, y = self.mixup(spectrogram, y)\n            \n            spectrogram = self.batch_crop(spectrogram)\n            spectrogram = spectrogram.permute(0, 2, 1, 3)\n                \n        x = self.backbone(spectrogram)\n        if self.training:\n            x = x.permute(0, 2, 1, 3)\n            x = self.batch_uncrop(x)\n            x = x.permute(0, 2, 1, 3)\n                \n        logits = self.head(x)\n        \n        if y is not None:\n            loss = self.loss(logits, y)\n        else:\n            loss = None\n\n        return {'loss': loss, 'logits': logits.sigmoid()}\n    \n    def batch_crop(self, tensor):\n        factor = int(CONFIG['crop_len'] // TEST_SIZE)\n        b, t = tensor.shape[:2]\n        tensor = tensor.reshape(b * factor, t // factor, *tensor.shape[2:])\n        return tensor\n    \n    def batch_uncrop(self, tensor):\n        factor = int(CONFIG['crop_len'] // TEST_SIZE)\n        b, t = tensor.shape[:2]\n        tensor = tensor.reshape(b // factor, t * factor, *tensor.shape[2:])\n        return tensor\n    \n    @staticmethod\n    def _init_audio2image():\n        mel = ta.transforms.MelSpectrogram(\n            sample_rate=32000,\n            n_fft=2048,\n            win_length=2048,\n            hop_length=512,\n            f_min=16,\n            f_max=16386,\n            pad=0,\n            n_mels=256,\n            power=2,\n            normalized=False,\n        )\n        db_scale = ta.transforms.AmplitudeToDB(top_db=80.0)\n        audio2image = torch.nn.Sequential(mel, db_scale)\n        return audio2image\n    \n    @staticmethod\n    def _init_backbone():\n        backbone = \"resnet18\"\n        pretrained = False\n        pretrained_weights = None\n        train = True\n        val = False\n        in_chans = 1\n\n        backbone = timm.create_model(\n            backbone,\n            pretrained=pretrained,\n            num_classes=0,\n            global_pool=\"\",\n            in_chans=in_chans,\n        )\n        return backbone\n    \n    @staticmethod\n    def _init_head(num_chs):\n        head = torch.nn.Sequential(\n            torch.nn.AdaptiveAvgPool2d(output_size=1),\n            torch.nn.Flatten(),\n            torch.nn.Linear(num_chs, len(all_species))\n        )\n        return head\n    \n    def load_backbone(self, weights_path=None):\n        if weights_path:\n            state_dict=torch.load(weights_path)\n            conv1_weight = state_dict['conv1.weight']\n            state_dict['conv1.weight'] = conv1_weight.sum(dim=1, keepdim=True)\n            state_dict.pop('fc.bias')\n            state_dict.pop('fc.weight')\n            self.backbone.load_state_dict(state_dict)\n        ","metadata":{"execution":{"iopub.status.busy":"2022-05-01T16:01:49.394775Z","iopub.execute_input":"2022-05-01T16:01:49.395336Z","iopub.status.idle":"2022-05-01T16:01:49.417869Z","shell.execute_reply.started":"2022-05-01T16:01:49.395293Z","shell.execute_reply":"2022-05-01T16:01:49.417026Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"### Model load","metadata":{}},{"cell_type":"code","source":"model_path = 'some_path'\nmodel = Net()\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nstate_dict=torch.load(model_path)\nmodel.load_state_dict(state_dict)\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2022-05-01T14:43:58.480859Z","iopub.execute_input":"2022-05-01T14:43:58.481375Z","iopub.status.idle":"2022-05-01T14:43:58.784505Z","shell.execute_reply.started":"2022-05-01T14:43:58.48134Z","shell.execute_reply":"2022-05-01T14:43:58.783768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Submit","metadata":{}},{"cell_type":"code","source":"with open(os.path.join(data_root, 'scored_birds.json')) as fin:\n    test_birds = json.load(fin)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T21:09:02.513487Z","iopub.execute_input":"2022-04-26T21:09:02.513766Z","iopub.status.idle":"2022-04-26T21:09:02.543443Z","shell.execute_reply.started":"2022-04-26T21:09:02.513736Z","shell.execute_reply":"2022-04-26T21:09:02.54281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TestDataset(Dataset):\n    def __init__(self, test_folder):\n        super().__init__()\n        self.test_folder = test_folder\n        self.fnames = [f for f in os.listdir(test_folder) if f.endswith('.ogg')]\n        \n    def __len__(self):\n        return len(self.fnames)\n    \n    def __getitem__(self, idx):\n        fpath = os.path.join(self.test_folder, self.fnames[idx])\n        wav, sr = load_wav(fpath, 0, None)\n        wav = torch.tensor(wav)\n        assert (13 * 5 * sr) > len(wav) \n        wav = wav[:len(wav) // 12 * 12].reshape((12, len(wav) // 12))\n        return wav","metadata":{"execution":{"iopub.status.busy":"2022-04-26T21:09:04.32293Z","iopub.execute_input":"2022-04-26T21:09:04.323721Z","iopub.status.idle":"2022-04-26T21:09:04.336316Z","shell.execute_reply.started":"2022-04-26T21:09:04.323676Z","shell.execute_reply":"2022-04-26T21:09:04.33538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = TestDataset(os.path.join(data_root, 'test_soundscapes'))\ntest_dataloader = DataLoader(\n    test_dataset,\n    batch_size=1,\n    shuffle=False,\n    num_workers=2,\n    drop_last=False,   \n)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T21:09:06.034536Z","iopub.execute_input":"2022-04-26T21:09:06.035122Z","iopub.status.idle":"2022-04-26T21:09:06.043876Z","shell.execute_reply.started":"2022-04-26T21:09:06.03508Z","shell.execute_reply":"2022-04-26T21:09:06.04304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_list = []\ntreshold = 0.1\nmodel.eval()\nwith torch.no_grad():\n    for i, batch in tqdm(enumerate(test_dataloader)):\n        batch_size, part_count, part_size = batch.shape\n        batch = batch.reshape(batch_size * part_count, part_size)\n        pred = model(batch.to(device))['logits']\n        pred = pred.cpu().numpy()\n        pred = pred > treshold\n        \n        for j, chunk_pred in enumerate(pred):\n            inbatch_number = j // part_count\n            chunk_number = j % part_count + 1\n            f_idx = i * batch_size + inbatch_number\n            fname = test_dataset.fnames[f_idx]\n            prefix = fname.split('.')[0]\n            sufix = f'{5 * chunk_number}'\n            \n            pred_list.extend([{\n                'row_id': '_'.join([prefix, b, sufix]),\n                'target': chunk_pred[species2id[b]]\n            } for b in test_birds])\npred_pd = pd.DataFrame(pred_list)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T21:09:22.267572Z","iopub.execute_input":"2022-04-26T21:09:22.267846Z","iopub.status.idle":"2022-04-26T21:09:22.504385Z","shell.execute_reply.started":"2022-04-26T21:09:22.267799Z","shell.execute_reply":"2022-04-26T21:09:22.503538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_pd.to_csv(\"submission.csv\", index=False)\npred_pd","metadata":{"execution":{"iopub.status.busy":"2022-04-26T21:09:23.724651Z","iopub.execute_input":"2022-04-26T21:09:23.725436Z","iopub.status.idle":"2022-04-26T21:09:23.740815Z","shell.execute_reply.started":"2022-04-26T21:09:23.725397Z","shell.execute_reply":"2022-04-26T21:09:23.740159Z"},"trusted":true},"execution_count":null,"outputs":[]}]}