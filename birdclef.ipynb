{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\nimport sys\nsys.path.append('../input/timm-pytorch-image-models/pytorch-image-models-master')\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom matplotlib import pyplot as plt\nimport json\n\nimport librosa\n\nimport torch\nimport torchaudio as ta\nimport timm\n\nfrom tqdm import tqdm\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-23T14:54:54.181626Z","iopub.execute_input":"2022-04-23T14:54:54.182133Z","iopub.status.idle":"2022-04-23T14:55:00.047318Z","shell.execute_reply.started":"2022-04-23T14:54:54.181988Z","shell.execute_reply":"2022-04-23T14:55:00.046157Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"data_root = '/kaggle/input/birdclef-2022'\ntrain_meta = pd.read_csv(os.path.join(data_root, 'train_metadata.csv'))\nebird_taxonomy = pd.read_csv(os.path.join(data_root, 'eBird_Taxonomy_v2021.csv'))","metadata":{"execution":{"iopub.status.busy":"2022-04-23T14:55:00.050335Z","iopub.execute_input":"2022-04-23T14:55:00.051019Z","iopub.status.idle":"2022-04-23T14:55:00.229718Z","shell.execute_reply.started":"2022-04-23T14:55:00.050956Z","shell.execute_reply":"2022-04-23T14:55:00.228536Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"train_meta.loc[:, 'secondary_labels'] = train_meta.secondary_labels.apply(eval)\ntrain_meta['target_raw'] = train_meta.secondary_labels + train_meta.primary_label.apply(lambda x: [x])","metadata":{"execution":{"iopub.status.busy":"2022-04-23T14:55:00.232106Z","iopub.execute_input":"2022-04-23T14:55:00.232854Z","iopub.status.idle":"2022-04-23T14:55:00.518283Z","shell.execute_reply.started":"2022-04-23T14:55:00.232739Z","shell.execute_reply":"2022-04-23T14:55:00.517182Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"all_species = sorted(set(train_meta.target_raw.sum()))\nspecies2id = {s: i for i, s in enumerate(all_species)}\nid2species = {i: s for i, s in enumerate(all_species)}\n\ntrain_meta['target'] = train_meta.target_raw.apply(lambda species: [int(s in species) for s in all_species])","metadata":{"execution":{"iopub.status.busy":"2022-04-23T14:55:00.521086Z","iopub.execute_input":"2022-04-23T14:55:00.521478Z","iopub.status.idle":"2022-04-23T14:55:01.535200Z","shell.execute_reply.started":"2022-04-23T14:55:00.521416Z","shell.execute_reply":"2022-04-23T14:55:01.534159Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def load_wav(fname, offset, duration):\n#     fname = 'afrsil1/XC125458.ogg'\n    fpath = os.path.join(data_root, 'train_audio', fname)\n    wav, sr = librosa.load(fpath, sr=None, duration=duration)\n    assert sr <= 32000, sr\n    return wav, sr","metadata":{"execution":{"iopub.status.busy":"2022-04-23T14:55:01.537100Z","iopub.execute_input":"2022-04-23T14:55:01.537428Z","iopub.status.idle":"2022-04-23T14:55:01.544607Z","shell.execute_reply.started":"2022-04-23T14:55:01.537382Z","shell.execute_reply":"2022-04-23T14:55:01.543326Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"%%time\nduration = 30\nsample_rate = 32000\n\nwav, sr = load_wav('afrsil1/XC125458.ogg', 5, duration)\nto_pad = duration * sample_rate - wav.shape[0]\n\nif to_pad > 0:\n    wav = np.pad(wav, (0, to_pad))\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-23T14:55:01.546728Z","iopub.execute_input":"2022-04-23T14:55:01.547099Z","iopub.status.idle":"2022-04-23T14:55:01.593490Z","shell.execute_reply.started":"2022-04-23T14:55:01.547051Z","shell.execute_reply":"2022-04-23T14:55:01.592487Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"CPU times: user 12.4 ms, sys: 5.07 ms, total: 17.5 ms\nWall time: 27.7 ms\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Torch Dataset","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\n\nclass BirdDataset(Dataset):\n    def __init__(self, df):\n        super().__init__()\n        self.df = df\n        \n    def __getitem__(self, idx):\n        duration = 30\n        sample_rate = 32000\n        \n        fname = self.df.iloc[idx]['filename']\n        # TODO: add random offset\n        wav, sr = load_wav(fname, 0, duration)\n        to_pad = duration * sample_rate - wav.shape[0]\n        if to_pad > 0:\n            wav = np.pad(wav, (0, to_pad))\n            \n        target = self.df.iloc[idx]['target']\n        \n        # TODO: add weighting\n            \n        wav = torch.tensor(wav)\n        target = torch.tensor(target, dtype=float)\n        return {\n            'wav': wav,\n            'target': target,\n        }\n\n    def __len__(self):\n        return len(self.df)","metadata":{"execution":{"iopub.status.busy":"2022-04-23T14:55:01.596100Z","iopub.execute_input":"2022-04-23T14:55:01.596447Z","iopub.status.idle":"2022-04-23T14:55:01.608205Z","shell.execute_reply.started":"2022-04-23T14:55:01.596396Z","shell.execute_reply":"2022-04-23T14:55:01.606998Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"### Model","metadata":{"execution":{"iopub.status.busy":"2022-04-21T09:17:01.036048Z","iopub.execute_input":"2022-04-21T09:17:01.036366Z","iopub.status.idle":"2022-04-21T09:17:01.05202Z","shell.execute_reply.started":"2022-04-21T09:17:01.036334Z","shell.execute_reply":"2022-04-21T09:17:01.050779Z"}}},{"cell_type":"code","source":"class Net(torch.nn.Module):\n    def __init__(self, backbone_path=None):\n        super().__init__()\n        self.audio2image = self._init_audio2image()\n        self.backbone = self._init_backbone()\n        self.load_backbone(backbone_path)\n        self.head = self._init_head(self.backbone.feature_info[-1]['num_chs'])      \n        self.loss = torch.nn.BCEWithLogitsLoss()\n        \n    def forward(self, wav_tensor, y=None):\n        spectrogram = self.audio2image(wav_tensor)\n        spectrogram = spectrogram.permute(0, 2, 1)\n        spectrogram = spectrogram[:, None, :, :]\n        x = self.backbone(spectrogram)\n        logits = self.head(x)\n        \n        if y:\n            loss = self.loss(logits, y)\n        else:\n            loss = None\n\n        return {'loss': loss, 'logits': logits.sigmoid()}\n\n    \n    @staticmethod\n    def _init_audio2image():\n        mel = ta.transforms.MelSpectrogram(\n            sample_rate=32000,\n            n_fft=2048,\n            win_length=2048,\n            hop_length=512,\n            f_min=16,\n            f_max=16386,\n            pad=0,\n            n_mels=256,\n            power=2,\n            normalized=False,\n        )\n        db_scale = ta.transforms.AmplitudeToDB(top_db=80.0)\n        audio2image = torch.nn.Sequential(mel, db_scale)\n        return audio2image\n    \n    @staticmethod\n    def _init_backbone():\n        backbone = \"resnet18\"\n        pretrained = False\n        pretrained_weights = None\n        train = True\n        val = False\n        in_chans = 1\n\n        backbone = timm.create_model(\n            backbone,\n            pretrained=pretrained,\n            num_classes=0,\n            global_pool=\"\",\n            in_chans=in_chans,\n        )\n        return backbone\n    \n    @staticmethod\n    def _init_head(num_chs):\n        head = torch.nn.Sequential(\n            torch.nn.AdaptiveAvgPool2d(output_size=1),\n            torch.nn.Flatten(),\n            torch.nn.Linear(num_chs, len(all_species))\n        )\n        return head\n    \n    def load_backbone(self, weights_path=None):\n        if weights_path:\n            state_dict=torch.load(weights_path)\n            conv1_weight = state_dict['conv1.weight']\n            state_dict['conv1.weight'] = conv1_weight.sum(dim=1, keepdim=True)\n            state_dict.pop('fc.bias')\n            state_dict.pop('fc.weight')\n            self.backbone.load_state_dict(state_dict)\n        ","metadata":{"execution":{"iopub.status.busy":"2022-04-23T14:55:03.740802Z","iopub.execute_input":"2022-04-23T14:55:03.741480Z","iopub.status.idle":"2022-04-23T14:55:03.758190Z","shell.execute_reply.started":"2022-04-23T14:55:03.741427Z","shell.execute_reply":"2022-04-23T14:55:03.757148Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"### Train loop","metadata":{}},{"cell_type":"code","source":"with open('../input/timm-pretrained-resnet/index.json') as fin:\n    timm_index = json.load(fin)\nresnet_path = os.path.join('../input/timm-pretrained-resnet/resnet', timm_index['resnet']['resnet18'])","metadata":{"execution":{"iopub.status.busy":"2022-04-23T14:55:05.915990Z","iopub.execute_input":"2022-04-23T14:55:05.916279Z","iopub.status.idle":"2022-04-23T14:55:05.929416Z","shell.execute_reply.started":"2022-04-23T14:55:05.916246Z","shell.execute_reply":"2022-04-23T14:55:05.928306Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"model = Net(resnet_path)\ntrain_dataset = BirdDataset(train_meta)\ntrain_dataloader = DataLoader(\n    train_dataset,\n    batch_size=64,\n    shuffle=True,\n    num_workers=2,\n    pin_memory=False,\n    drop_last=True,\n)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\noptimizer = torch.optim.Adam(model.parameters())","metadata":{"execution":{"iopub.status.busy":"2022-04-23T14:55:25.565424Z","iopub.execute_input":"2022-04-23T14:55:25.565815Z","iopub.status.idle":"2022-04-23T14:55:29.460021Z","shell.execute_reply.started":"2022-04-23T14:55:25.565778Z","shell.execute_reply":"2022-04-23T14:55:29.459025Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def train_epoch(model, optimizer, dataloader, device):\n    tqdm_dataloader = tqdm(dataloader)\n    loss_list = []\n    for batch in tqdm_dataloader:\n        loss = model(batch['wav'].to(device), batch['target'].to(device))['loss']\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        loss_list.append(loss.item())\n    return loss_list\n    ","metadata":{"execution":{"iopub.status.busy":"2022-04-23T14:55:31.870495Z","iopub.execute_input":"2022-04-23T14:55:31.870796Z","iopub.status.idle":"2022-04-23T14:55:31.877422Z","shell.execute_reply.started":"2022-04-23T14:55:31.870764Z","shell.execute_reply":"2022-04-23T14:55:31.876254Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"if False:\n    epochs = 2\n    model.train()\n    model.to(device)\n    for e in range(epochs):\n        epoch_loss = train_epoch(model, optimizer, train_dataloader, device)\n        print(f'{e} train loss:', f'{epoch_loss:.3f}', sep='\\t')","metadata":{"execution":{"iopub.status.busy":"2022-04-23T14:55:33.965708Z","iopub.execute_input":"2022-04-23T14:55:33.966511Z","iopub.status.idle":"2022-04-23T14:55:33.973561Z","shell.execute_reply.started":"2022-04-23T14:55:33.966474Z","shell.execute_reply":"2022-04-23T14:55:33.972452Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"### Submit","metadata":{}},{"cell_type":"code","source":"with open(os.path.join(data_root, 'scored_birds.json')) as fin:\n    test_birds = json.load(fin)","metadata":{"execution":{"iopub.status.busy":"2022-04-23T14:55:36.231238Z","iopub.execute_input":"2022-04-23T14:55:36.232258Z","iopub.status.idle":"2022-04-23T14:55:36.241830Z","shell.execute_reply.started":"2022-04-23T14:55:36.232219Z","shell.execute_reply":"2022-04-23T14:55:36.240807Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"class TestDataset(Dataset):\n    def __init__(self, test_folder):\n        super().__init__()\n        self.test_folder = test_folder\n        self.fnames = [f for f in os.listdir(test_folder) if f.endswith('.ogg')]\n        \n    def __len__(self):\n        return len(self.fnames)\n    \n    def __getitem__(self, idx):\n        fpath = os.path.join(self.test_folder, self.fnames[idx])\n        wav, sr = load_wav(fpath, 0, None)\n        wav = torch.tensor(wav)\n        assert (13 * 5 * sr) > len(wav) \n        wav = wav[:len(wav) // 12 * 12].reshape((12, len(wav) // 12))\n        return wav","metadata":{"execution":{"iopub.status.busy":"2022-04-23T15:15:55.393012Z","iopub.execute_input":"2022-04-23T15:15:55.393501Z","iopub.status.idle":"2022-04-23T15:15:55.403617Z","shell.execute_reply.started":"2022-04-23T15:15:55.393455Z","shell.execute_reply":"2022-04-23T15:15:55.402269Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"test_dataset = TestDataset(os.path.join(data_root, 'test_soundscapes'))\ntest_dataloader = DataLoader(\n    test_dataset,\n    batch_size=1,\n    shuffle=False,\n    num_workers=2,\n    drop_last=False,   \n)","metadata":{"execution":{"iopub.status.busy":"2022-04-23T15:15:55.741731Z","iopub.execute_input":"2022-04-23T15:15:55.742591Z","iopub.status.idle":"2022-04-23T15:15:55.751021Z","shell.execute_reply.started":"2022-04-23T15:15:55.742553Z","shell.execute_reply":"2022-04-23T15:15:55.749923Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"pred_list = []\ntreshold = 0.5\nmodel.eval()\nwith torch.no_grad():\n    for i, batch in tqdm(enumerate(test_dataloader)):\n        batch_size, part_count, part_size = batch.shape\n        batch = batch.reshape(batch_size * part_count, part_size)\n        pred = model(batch.to(device))['logits']\n        pred = pred.cpu().numpy()\n        pred = pred > treshold\n        \n        for j, chunk_pred in enumerate(pred):\n            inbatch_number = j // part_count\n            chunk_number = j % part_count + 1\n            f_idx = i * batch_size + inbatch_number\n            fname = test_dataset.fnames[f_idx]\n            prefix = fname.split('.')[0]\n            sufix = f'{5 * chunk_number}'\n            \n            pred_list.extend([{\n                'row_id': '_'.join([prefix, b, sufix]),\n                'target': chunk_pred[species2id[b]]\n            } for b in test_birds])\npred_pd = pd.DataFrame(pred_list)","metadata":{"execution":{"iopub.status.busy":"2022-04-23T15:15:56.359515Z","iopub.execute_input":"2022-04-23T15:15:56.359828Z","iopub.status.idle":"2022-04-23T15:15:56.624536Z","shell.execute_reply.started":"2022-04-23T15:15:56.359794Z","shell.execute_reply":"2022-04-23T15:15:56.623346Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stderr","text":"1it [00:00,  5.93it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"pred_pd.to_csv(\"submission.csv\", index=False)\npred_pd","metadata":{"execution":{"iopub.status.busy":"2022-04-23T15:15:58.596742Z","iopub.execute_input":"2022-04-23T15:15:58.597635Z","iopub.status.idle":"2022-04-23T15:15:58.619723Z","shell.execute_reply.started":"2022-04-23T15:15:58.597579Z","shell.execute_reply":"2022-04-23T15:15:58.618792Z"},"trusted":true},"execution_count":48,"outputs":[{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"                              row_id  target\n0      soundscape_453028782_akiapo_5   False\n1      soundscape_453028782_aniani_5   False\n2      soundscape_453028782_apapan_5    True\n3      soundscape_453028782_barpet_5    True\n4      soundscape_453028782_crehon_5    True\n..                               ...     ...\n247     soundscape_453028782_omao_60    True\n248   soundscape_453028782_puaioh_60   False\n249   soundscape_453028782_skylar_60   False\n250  soundscape_453028782_warwhe1_60    True\n251   soundscape_453028782_yefcan_60   False\n\n[252 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>row_id</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>soundscape_453028782_akiapo_5</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>soundscape_453028782_aniani_5</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>soundscape_453028782_apapan_5</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>soundscape_453028782_barpet_5</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>soundscape_453028782_crehon_5</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>247</th>\n      <td>soundscape_453028782_omao_60</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>248</th>\n      <td>soundscape_453028782_puaioh_60</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>249</th>\n      <td>soundscape_453028782_skylar_60</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>250</th>\n      <td>soundscape_453028782_warwhe1_60</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>251</th>\n      <td>soundscape_453028782_yefcan_60</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n<p>252 rows Ã— 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# def chunk_wav(wav, sr, window_size):\n#     chunks_count = len(wav) // (window_size * sr)\n#     chunk_size = window_size * sr\n#     chunks = []\n#     for chunk_idx in range(chunks_count):\n#         left = chunk_idx * sr\n#         right = min(left + chunk_size, len(wav))\n#         chunks.append(wav[left:right])\n#     chunk_tensor = torch.tensor(chunks)\n#     return chunk_tensor\n\n# file_list = os.listdir(os.path.join(data_root, 'test_soundscapes'))\n\n# # This is where we will store our results\n# treshold = 0.5\n# pred = {'row_id': [], 'target': []}\n# model.eval()\n# with torch.no_grad():\n#     # Process audio files and make predictions\n#     for fname in file_list:\n#         prefix = fname.split('.')[0]\n#         # Complete file path\n#         fpath = os.path.join(data_root, 'test_soundscapes', fname)\n#         wav, sr = load_wav(fpath, 0, None)\n#         chunk_tensor = chunk_wav(wav, sr, window_size=5)\n\n#         # Open file with librosa and split signal into 5-second chunks\n#         # sig, rate = librosa.load(path)\n#         # ...\n\n#         # Let's assume we have a list of 12 audio chunks (1min / 5s == 12 segments)\n#         chunk_score = model(chunk_tensor.to(device))['logits'].cpu().numpy()\n\n#         # Make prediction for each chunk\n#         # Each scored bird gets a random value in our case\n#         # since we don't actually have a model\n#         for i, all_score in enumerate(chunk_score):        \n#             chunk_end_time = (i + 1) * 5\n#             for bird in test_birds:\n\n#                 # This is our random prediction score for this bird\n#                 bird_score = all_score[species2id[bird]]\n\n#                 # Assemble the row_id which we need to do for each scored bird\n#                 row_id = prefix + '_' + bird + '_' + str(chunk_end_time)\n\n#                 # Put the result into our prediction dict and\n#                 # apply a \"confidence\" threshold of 0.5\n#                 pred['row_id'].append(row_id)\n#                 pred['target'].append(True if bird_score > treshold else False)\n\n\n# # Make a new data frame and look at some results        \n# results = pd.DataFrame(pred, columns = ['row_id', 'target'])\n\n# # Quick sanity check\n# print(results.head()) \n    \n# # Convert our results to csv\n# results.to_csv(\"submission.csv\", index=False)    ","metadata":{"execution":{"iopub.status.busy":"2022-04-23T15:00:22.026005Z","iopub.execute_input":"2022-04-23T15:00:22.026332Z","iopub.status.idle":"2022-04-23T15:00:22.458590Z","shell.execute_reply.started":"2022-04-23T15:00:22.026291Z","shell.execute_reply":"2022-04-23T15:00:22.457601Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# test_fnames = [f for f in os.listdir(f'{data_root}/test_soundscapes') if f.endswith('.ogg')]\n# test_pd = []\n# for fname in test_fnames:\n#     fpath = os.path.join(data_root, 'test_soundscapes', fname)\n#     wav, sr = librosa.load(fpath, sr=None)\n#     prefix = fname.split('.')[0]\n#     window_size = 5 * sr\n#     for i, chunk in enumerate(wav[::window_size]):\n#         end_time = (i + 1) * 5\n#         samples = [{\n#             'row_id': f'{prefix}_{b}_{end_time}',\n#             'file_id': fname,\n#             'bird': b,\n#             'end_time': end_time\n#         } for b in test_birds]\n#         test_pd.extend(samples)\n        \n# test_pd = pd.DataFrame(test_pd)\n# test_pd","metadata":{},"execution_count":null,"outputs":[]}]}