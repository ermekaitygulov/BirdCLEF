{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\nimport sys\nsys.path.append('../input/timm-pytorch-image-models/pytorch-image-models-master')\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom matplotlib import pyplot as plt\nimport json\nimport random\n\nimport librosa\n\nimport torch\nimport torchaudio as ta\nimport timm\n\nfrom tqdm import tqdm\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-01T13:59:52.355979Z","iopub.execute_input":"2022-05-01T13:59:52.356485Z","iopub.status.idle":"2022-05-01T13:59:52.363193Z","shell.execute_reply.started":"2022-05-01T13:59:52.356423Z","shell.execute_reply":"2022-05-01T13:59:52.362568Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"data_root = '/kaggle/input/birdclef-2022'\ntrain_meta = pd.read_csv('../input/birdclef-data-with-wav-durations/train_metadata_extended.csv')\nebird_taxonomy = pd.read_csv(os.path.join(data_root, 'eBird_Taxonomy_v2021.csv'))","metadata":{"execution":{"iopub.status.busy":"2022-05-01T13:56:45.465535Z","iopub.execute_input":"2022-05-01T13:56:45.465783Z","iopub.status.idle":"2022-05-01T13:56:45.621221Z","shell.execute_reply.started":"2022-05-01T13:56:45.465756Z","shell.execute_reply":"2022-05-01T13:56:45.620509Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"train_meta.loc[:, 'secondary_labels'] = train_meta.secondary_labels.apply(eval)\ntrain_meta['target_raw'] = train_meta.secondary_labels + train_meta.primary_label.apply(lambda x: [x])","metadata":{"execution":{"iopub.status.busy":"2022-05-01T13:56:47.113043Z","iopub.execute_input":"2022-05-01T13:56:47.115951Z","iopub.status.idle":"2022-05-01T13:56:47.398945Z","shell.execute_reply.started":"2022-05-01T13:56:47.115906Z","shell.execute_reply":"2022-05-01T13:56:47.398229Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"all_species = sorted(set(train_meta.target_raw.sum()))\nspecies2id = {s: i for i, s in enumerate(all_species)}\nid2species = {i: s for i, s in enumerate(all_species)}\n\ntrain_meta['target'] = train_meta.target_raw.apply(lambda species: [int(s in species) for s in all_species])","metadata":{"execution":{"iopub.status.busy":"2022-05-01T13:56:54.152995Z","iopub.execute_input":"2022-05-01T13:56:54.153283Z","iopub.status.idle":"2022-05-01T13:56:54.903213Z","shell.execute_reply.started":"2022-05-01T13:56:54.153252Z","shell.execute_reply":"2022-05-01T13:56:54.902507Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def load_wav(fname, offset, duration):\n#     fname = 'afrsil1/XC125458.ogg'\n    fpath = os.path.join(data_root, 'train_audio', fname)\n    wav, sr = librosa.load(fpath, sr=None, duration=duration)\n    assert sr <= 32000, sr\n    return wav, sr","metadata":{"execution":{"iopub.status.busy":"2022-05-01T13:56:56.825379Z","iopub.execute_input":"2022-05-01T13:56:56.826044Z","iopub.status.idle":"2022-05-01T13:56:56.830879Z","shell.execute_reply.started":"2022-05-01T13:56:56.826008Z","shell.execute_reply":"2022-05-01T13:56:56.830072Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# %%time\n# duration = 30\n# sample_rate = 32000\n\n# wav, sr = load_wav('afrsil1/XC125458.ogg', 5, duration)\n# to_pad = duration * sample_rate - wav.shape[0]\n\n# if to_pad > 0:\n#     wav = np.pad(wav, (0, to_pad))\n\n","metadata":{"execution":{"iopub.status.busy":"2022-05-01T13:56:59.065740Z","iopub.execute_input":"2022-05-01T13:56:59.066238Z","iopub.status.idle":"2022-05-01T13:56:59.069568Z","shell.execute_reply.started":"2022-05-01T13:56:59.066200Z","shell.execute_reply":"2022-05-01T13:56:59.068794Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"### Torch Dataset","metadata":{}},{"cell_type":"code","source":"TEST_SIZE = 5 \nCONFIG = {\n    'crop_len': 30,\n    'sample_rate': 32000,    \n}\n","metadata":{"execution":{"iopub.status.busy":"2022-05-01T14:16:25.201077Z","iopub.execute_input":"2022-05-01T14:16:25.201327Z","iopub.status.idle":"2022-05-01T14:16:25.205380Z","shell.execute_reply.started":"2022-05-01T14:16:25.201298Z","shell.execute_reply":"2022-05-01T14:16:25.204343Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\n\nclass BirdDataset(Dataset):\n    def __init__(self, df):\n        super().__init__()\n        self.df = df\n        \n    def __getitem__(self, idx):\n        duration = CONFIG['crop_len']\n        sample_rate = CONFIG['sample_rate']\n        \n        fname = self.df.iloc[idx]['filename']\n        wav_len = train_meta.iloc[0]['duration']\n        \n        max_offset = max(0, wav_len - duration)\n        random_offset = random.randint(0, max_offset)\n                \n        wav, sr = load_wav(fname, random_offset, duration)\n        to_pad = duration * sample_rate - wav.shape[0]\n        if to_pad > 0:\n            wav = np.pad(wav, (0, to_pad))\n            \n        target = self.df.iloc[idx]['target']\n        \n        # TODO: add weighting\n            \n        wav = torch.tensor(wav)\n        target = torch.tensor(target, dtype=float)\n        return {\n            'wav': wav,\n            'target': target,\n        }\n\n    def __len__(self):\n        return len(self.df)","metadata":{"execution":{"iopub.status.busy":"2022-05-01T14:03:15.786208Z","iopub.execute_input":"2022-05-01T14:03:15.786482Z","iopub.status.idle":"2022-05-01T14:03:15.797423Z","shell.execute_reply.started":"2022-05-01T14:03:15.786439Z","shell.execute_reply":"2022-05-01T14:03:15.794893Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"### Model","metadata":{"execution":{"iopub.status.busy":"2022-04-21T09:17:01.036048Z","iopub.execute_input":"2022-04-21T09:17:01.036366Z","iopub.status.idle":"2022-04-21T09:17:01.05202Z","shell.execute_reply.started":"2022-04-21T09:17:01.036334Z","shell.execute_reply":"2022-04-21T09:17:01.050779Z"}}},{"cell_type":"code","source":"from torch.distributions import Beta\n\n\nclass Mixup(torch.nn.Module):\n    def __init__(self, mix_beta=1):\n\n        super(Mixup, self).__init__()\n        self.beta_distribution = Beta(mix_beta, mix_beta)\n\n    def forward(self, X, Y, weight=None):\n\n        bs = X.shape[0]\n        n_dims = len(X.shape)\n        perm = torch.randperm(bs)\n        coeffs = self.beta_distribution.rsample(torch.Size((bs,))).to(X.device)\n\n        if n_dims == 2:\n            X = coeffs.view(-1, 1) * X + (1 - coeffs.view(-1, 1)) * X[perm]\n        elif n_dims == 3:\n            X = coeffs.view(-1, 1, 1) * X + (1 - coeffs.view(-1, 1, 1)) * X[perm]\n        else:\n            X = coeffs.view(-1, 1, 1, 1) * X + (1 - coeffs.view(-1, 1, 1, 1)) * X[perm]\n\n        Y = coeffs.view(-1, 1) * Y + (1 - coeffs.view(-1, 1)) * Y[perm]\n\n        if weight is None:\n            return X, Y\n        else:\n            weight = coeffs.view(-1) * weight + (1 - coeffs.view(-1)) * weight[perm]\n            return X, Y, weight","metadata":{"execution":{"iopub.status.busy":"2022-05-01T14:28:46.596701Z","iopub.execute_input":"2022-05-01T14:28:46.596987Z","iopub.status.idle":"2022-05-01T14:28:46.607320Z","shell.execute_reply.started":"2022-05-01T14:28:46.596956Z","shell.execute_reply":"2022-05-01T14:28:46.606307Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"class Net(torch.nn.Module):\n    def __init__(self, backbone_path=None):\n        super().__init__()\n        self.audio2image = self._init_audio2image()\n        self.backbone = self._init_backbone()\n        self.load_backbone(backbone_path)\n        self.head = self._init_head(self.backbone.feature_info[-1]['num_chs'])      \n        self.loss = torch.nn.BCEWithLogitsLoss()\n        self.mixup = Mixup()\n        \n    def forward(self, wav_tensor, y=None):\n        if self.training:\n            wav_tensor = self.batch_crop(wav_tensor)\n            \n        spectrogram = self.audio2image(wav_tensor)\n        spectrogram = spectrogram.permute(0, 2, 1)\n        spectrogram = spectrogram[:, None, :, :]\n        \n        if self.training:\n            spectrogram = spectrogram.permute(0, 2, 1, 3)\n            spectrogram = self.batch_uncrop(spectrogram)\n            \n            spectrogram, y = self.mixup(spectrogram, y)\n            \n            spectrogram = self.batch_crop(spectrogram)\n            spectrogram = spectrogram.permute(0, 2, 1, 3)\n                \n        x = self.backbone(spectrogram)\n        if self.training:\n            x = x.permute(0, 2, 1, 3)\n            x = self.batch_uncrop(x)\n            x = x.permute(0, 2, 1, 3)\n                \n        logits = self.head(x)\n        \n        if y is not None:\n            loss = self.loss(logits, y)\n        else:\n            loss = None\n\n        return {'loss': loss, 'logits': logits.sigmoid()}\n    \n    def batch_crop(self, tensor):\n        factor = int(CONFIG['crop_len'] // TEST_SIZE)\n        b, t = tensor.shape[:2]\n        tensor = tensor.reshape(b * factor, t // factor, *tensor.shape[2:])\n        return tensor\n    \n    def batch_uncrop(self, tensor):\n        factor = int(CONFIG['crop_len'] // TEST_SIZE)\n        b, t = tensor.shape[:2]\n        tensor = tensor.reshape(b // factor, t * factor, *tensor.shape[2:])\n        return tensor\n    \n    @staticmethod\n    def _init_audio2image():\n        mel = ta.transforms.MelSpectrogram(\n            sample_rate=32000,\n            n_fft=2048,\n            win_length=2048,\n            hop_length=512,\n            f_min=16,\n            f_max=16386,\n            pad=0,\n            n_mels=256,\n            power=2,\n            normalized=False,\n        )\n        db_scale = ta.transforms.AmplitudeToDB(top_db=80.0)\n        audio2image = torch.nn.Sequential(mel, db_scale)\n        return audio2image\n    \n    @staticmethod\n    def _init_backbone():\n        backbone = \"resnet18\"\n        pretrained = False\n        pretrained_weights = None\n        train = True\n        val = False\n        in_chans = 1\n\n        backbone = timm.create_model(\n            backbone,\n            pretrained=pretrained,\n            num_classes=0,\n            global_pool=\"\",\n            in_chans=in_chans,\n        )\n        return backbone\n    \n    @staticmethod\n    def _init_head(num_chs):\n        head = torch.nn.Sequential(\n            torch.nn.AdaptiveAvgPool2d(output_size=1),\n            torch.nn.Flatten(),\n            torch.nn.Linear(num_chs, len(all_species))\n        )\n        return head\n    \n    def load_backbone(self, weights_path=None):\n        if weights_path:\n            state_dict=torch.load(weights_path)\n            conv1_weight = state_dict['conv1.weight']\n            state_dict['conv1.weight'] = conv1_weight.sum(dim=1, keepdim=True)\n            state_dict.pop('fc.bias')\n            state_dict.pop('fc.weight')\n            self.backbone.load_state_dict(state_dict)\n        ","metadata":{"execution":{"iopub.status.busy":"2022-05-01T14:43:55.884145Z","iopub.execute_input":"2022-05-01T14:43:55.884424Z","iopub.status.idle":"2022-05-01T14:43:55.904214Z","shell.execute_reply.started":"2022-05-01T14:43:55.884393Z","shell.execute_reply":"2022-05-01T14:43:55.903345Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"markdown","source":"### Train loop","metadata":{}},{"cell_type":"code","source":"with open('../input/timm-pretrained-resnet/index.json') as fin:\n    timm_index = json.load(fin)\nresnet_path = os.path.join('../input/timm-pretrained-resnet/resnet', timm_index['resnet']['resnet18'])","metadata":{"execution":{"iopub.status.busy":"2022-05-01T14:38:44.001013Z","iopub.execute_input":"2022-05-01T14:38:44.001653Z","iopub.status.idle":"2022-05-01T14:38:44.010592Z","shell.execute_reply.started":"2022-05-01T14:38:44.001616Z","shell.execute_reply":"2022-05-01T14:38:44.009913Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score, balanced_accuracy_score","metadata":{"execution":{"iopub.status.busy":"2022-05-01T14:40:02.001436Z","iopub.execute_input":"2022-05-01T14:40:02.002128Z","iopub.status.idle":"2022-05-01T14:40:02.005775Z","shell.execute_reply.started":"2022-05-01T14:40:02.002092Z","shell.execute_reply":"2022-05-01T14:40:02.004840Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"train_meta, val_meta = train_test_split(train_meta, test_size=0.2)","metadata":{"execution":{"iopub.status.busy":"2022-05-01T14:38:47.360254Z","iopub.execute_input":"2022-05-01T14:38:47.360517Z","iopub.status.idle":"2022-05-01T14:38:47.372573Z","shell.execute_reply.started":"2022-05-01T14:38:47.360487Z","shell.execute_reply":"2022-05-01T14:38:47.371834Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"model = Net(resnet_path)\ntrain_dataset = BirdDataset(train_meta)\ntrain_dataloader = DataLoader(\n    train_dataset,\n    batch_size=64,\n    shuffle=True,\n    num_workers=2,\n    pin_memory=False,\n    drop_last=True,\n)\n\nval_dataset = BirdDataset(val_meta)\nval_dataloader = DataLoader(\n    val_dataset,\n    batch_size=64,\n    shuffle=False,\n    num_workers=2,\n    pin_memory=False,\n    drop_last=False,\n)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\noptimizer = torch.optim.Adam(model.parameters())","metadata":{"execution":{"iopub.status.busy":"2022-05-01T14:43:58.480859Z","iopub.execute_input":"2022-05-01T14:43:58.481375Z","iopub.status.idle":"2022-05-01T14:43:58.784505Z","shell.execute_reply.started":"2022-05-01T14:43:58.481340Z","shell.execute_reply":"2022-05-01T14:43:58.783768Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"def train_epoch(model, optimizer, dataloader, device):\n    tqdm_dataloader = tqdm(dataloader)\n    loss_list = []\n    model.train()\n    for batch in tqdm_dataloader:\n        loss = model(batch['wav'].to(device), batch['target'].to(device))['loss']\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        loss_list.append(loss.item())\n    return loss_list\n    \n\ndef val_epoch(model, dataloader, device):\n    tqdm_dataloader = tqdm(dataloader)\n    loss_list = []\n    model.eval()\n    y_true = None\n    y_pred = None\n    \n    for batch in tqdm_dataloader:\n        logits = model(batch['wav'].to(device))['logits']\n        batch_target = batch['target'].cpu().numpy()\n        batch_pred = logits.cpu().numpy()\n        \n        if y_true is None:\n            y_true = batch_target\n            y_pred = batch_pred\n        else:\n            y_true = np.vstack((y_true, batch_target))\n            y_pred = np.vstack((y_pred, batch_pred))\n        \n    return y_true, y_pred\n    ","metadata":{"execution":{"iopub.status.busy":"2022-05-01T14:38:55.061799Z","iopub.execute_input":"2022-05-01T14:38:55.062445Z","iopub.status.idle":"2022-05-01T14:38:55.072082Z","shell.execute_reply.started":"2022-05-01T14:38:55.062405Z","shell.execute_reply":"2022-05-01T14:38:55.071402Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"def score_pred(y_true, y_pred, trsh, score_conf):\n    score_dict = {}\n    for score_f, score_kwargs, score_prefix in score_conf:\n        score_dict.update({\n            f'{score_prefix}-{t}': score_f(y_true, y_pred > t, **score_kwargs)\n            for t in trsh\n        })\n    return score_dict\n\n\nimport sklearn.metrics\n\ndef comp_metric(y_true, y_pred, epsilon=1e-9):\n    \"\"\" Function to calculate competition metric in an sklearn like fashion\n\n    Args:\n        y_true{array-like, sparse matrix} of shape (n_samples, n_outputs)\n            - Ground truth (correct) target values.\n        y_pred{array-like, sparse matrix} of shape (n_samples, n_outputs)\n            - Estimated targets as returned by a classifier.\n    Returns:\n        The single calculated score representative of this competitions evaluation\n    \"\"\"\n\n    # Get representative confusion matrices for each label\n    mlbl_cms = sklearn.metrics.multilabel_confusion_matrix(y_true, y_pred)\n\n    # Get two scores (TP and TN SCORES)\n    tp_scores = np.array([\n        mlbl_cm[1, 1]/(epsilon+mlbl_cm[:, 1].sum()) \\\n        for mlbl_cm in mlbl_cms\n        ])\n    tn_scores = np.array([\n        mlbl_cm[0, 0]/(epsilon+mlbl_cm[:, 0].sum()) \\\n        for mlbl_cm in mlbl_cms\n        ])\n\n    # Get average\n    tp_mean = tp_scores.mean()\n    tn_mean = tn_scores.mean()\n\n    return round((tp_mean+tn_mean)/2, 8)\n\n\ndef balanced_accuracy(pred, target, eps=1e-6):\n    tp = (pred * target).sum(axis=-1)\n    fn = ((1 - pred) * target).sum(axis=-1)\n    fp = (pred * (1 - target)).sum(axis=-1)\n    tn = ((1 - pred) * (1 - target)).sum(axis=-1)\n    tpr = tp / (tp + fn + eps)\n    tnr = tn / (tn + fp + eps)\n    return (0.5 * (tpr + tnr)).mean()","metadata":{"execution":{"iopub.status.busy":"2022-05-01T14:56:35.241053Z","iopub.execute_input":"2022-05-01T14:56:35.241354Z","iopub.status.idle":"2022-05-01T14:56:35.251934Z","shell.execute_reply.started":"2022-05-01T14:56:35.241321Z","shell.execute_reply":"2022-05-01T14:56:35.251228Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"score_conf = [\n    [f1_score, {'average': 'macro'}, 'f1'],\n    [comp_metric, {}, 'comp_metric'],\n    [balanced_accuracy, {}, 'balanced_accuracy']\n]","metadata":{"execution":{"iopub.status.busy":"2022-05-01T14:56:36.326628Z","iopub.execute_input":"2022-05-01T14:56:36.327150Z","iopub.status.idle":"2022-05-01T14:56:36.331958Z","shell.execute_reply.started":"2022-05-01T14:56:36.327114Z","shell.execute_reply":"2022-05-01T14:56:36.330915Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"score_dict = score_pred(\n        y_true, y_pred,\n        score_conf=score_conf,\n        trsh={0.3, 0.4, 0.5, 0.6}\n    )\nscore_dict","metadata":{"execution":{"iopub.status.busy":"2022-05-01T14:56:36.836292Z","iopub.execute_input":"2022-05-01T14:56:36.836965Z","iopub.status.idle":"2022-05-01T14:56:37.120010Z","shell.execute_reply.started":"2022-05-01T14:56:36.836925Z","shell.execute_reply":"2022-05-01T14:56:37.119306Z"},"trusted":true},"execution_count":64,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1570: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1570: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1570: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1570: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n","output_type":"stream"},{"execution_count":64,"output_type":"execute_result","data":{"text/plain":"{'f1-0.3': 0.023010348113613497,\n 'f1-0.4': 0.020569173502477743,\n 'f1-0.6': 0.015787227486987437,\n 'f1-0.5': 0.01909760703733796,\n 'comp_metric-0.3': 0.51555145,\n 'comp_metric-0.4': 0.51757528,\n 'comp_metric-0.6': 0.51462809,\n 'comp_metric-0.5': 0.52014292,\n 'balanced_accuracy-0.3': 0.5431893967468772,\n 'balanced_accuracy-0.4': 0.5354139651285253,\n 'balanced_accuracy-0.6': 0.5243097573152996,\n 'balanced_accuracy-0.5': 0.5299882351412454}"},"metadata":{}}]},{"cell_type":"code","source":"epochs = 10\nmodel.to(device)\nfor e in range(epochs):\n    epoch_loss = train_epoch(model, optimizer, train_dataloader, device)\n    print(f'{e} train loss:', f'{np.mean(epoch_loss):.3f}', sep='\\t')\n    with torch.no_grad():\n        y_true, y_pred = val_epoch(model, val_dataloader, device)\n    score_dict = score_pred(\n        y_true, y_pred,\n        score_conf=score_conf,\n        trsh={0.3, 0.4, 0.5, 0.6}\n    )\n    torch.save(model, f'{e}_model.pt')\n    print(f'{e} val scores:')\n    print(*[\n        f'\\t{case}: {case_score}' \n        for case, case_score in score_dict.items()\n    ], sep='\\n')\n","metadata":{"execution":{"iopub.status.busy":"2022-05-01T14:56:48.517788Z","iopub.execute_input":"2022-05-01T14:56:48.518037Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":" 86%|████████▋ | 160/185 [06:19<00:52,  2.12s/it]","output_type":"stream"}]},{"cell_type":"markdown","source":"### Submit","metadata":{}},{"cell_type":"code","source":"with open(os.path.join(data_root, 'scored_birds.json')) as fin:\n    test_birds = json.load(fin)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T21:09:02.513487Z","iopub.execute_input":"2022-04-26T21:09:02.513766Z","iopub.status.idle":"2022-04-26T21:09:02.543443Z","shell.execute_reply.started":"2022-04-26T21:09:02.513736Z","shell.execute_reply":"2022-04-26T21:09:02.54281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TestDataset(Dataset):\n    def __init__(self, test_folder):\n        super().__init__()\n        self.test_folder = test_folder\n        self.fnames = [f for f in os.listdir(test_folder) if f.endswith('.ogg')]\n        \n    def __len__(self):\n        return len(self.fnames)\n    \n    def __getitem__(self, idx):\n        fpath = os.path.join(self.test_folder, self.fnames[idx])\n        wav, sr = load_wav(fpath, 0, None)\n        wav = torch.tensor(wav)\n        assert (13 * 5 * sr) > len(wav) \n        wav = wav[:len(wav) // 12 * 12].reshape((12, len(wav) // 12))\n        return wav","metadata":{"execution":{"iopub.status.busy":"2022-04-26T21:09:04.32293Z","iopub.execute_input":"2022-04-26T21:09:04.323721Z","iopub.status.idle":"2022-04-26T21:09:04.336316Z","shell.execute_reply.started":"2022-04-26T21:09:04.323676Z","shell.execute_reply":"2022-04-26T21:09:04.33538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = TestDataset(os.path.join(data_root, 'test_soundscapes'))\ntest_dataloader = DataLoader(\n    test_dataset,\n    batch_size=1,\n    shuffle=False,\n    num_workers=2,\n    drop_last=False,   \n)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T21:09:06.034536Z","iopub.execute_input":"2022-04-26T21:09:06.035122Z","iopub.status.idle":"2022-04-26T21:09:06.043876Z","shell.execute_reply.started":"2022-04-26T21:09:06.03508Z","shell.execute_reply":"2022-04-26T21:09:06.04304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_list = []\ntreshold = 0.1\nmodel.eval()\nwith torch.no_grad():\n    for i, batch in tqdm(enumerate(test_dataloader)):\n        batch_size, part_count, part_size = batch.shape\n        batch = batch.reshape(batch_size * part_count, part_size)\n        pred = model(batch.to(device))['logits']\n        pred = pred.cpu().numpy()\n        pred = pred > treshold\n        \n        for j, chunk_pred in enumerate(pred):\n            inbatch_number = j // part_count\n            chunk_number = j % part_count + 1\n            f_idx = i * batch_size + inbatch_number\n            fname = test_dataset.fnames[f_idx]\n            prefix = fname.split('.')[0]\n            sufix = f'{5 * chunk_number}'\n            \n            pred_list.extend([{\n                'row_id': '_'.join([prefix, b, sufix]),\n                'target': chunk_pred[species2id[b]]\n            } for b in test_birds])\npred_pd = pd.DataFrame(pred_list)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T21:09:22.267572Z","iopub.execute_input":"2022-04-26T21:09:22.267846Z","iopub.status.idle":"2022-04-26T21:09:22.504385Z","shell.execute_reply.started":"2022-04-26T21:09:22.267799Z","shell.execute_reply":"2022-04-26T21:09:22.503538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_pd.to_csv(\"submission.csv\", index=False)\npred_pd","metadata":{"execution":{"iopub.status.busy":"2022-04-26T21:09:23.724651Z","iopub.execute_input":"2022-04-26T21:09:23.725436Z","iopub.status.idle":"2022-04-26T21:09:23.740815Z","shell.execute_reply.started":"2022-04-26T21:09:23.725397Z","shell.execute_reply":"2022-04-26T21:09:23.740159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def chunk_wav(wav, sr, window_size):\n#     chunks_count = len(wav) // (window_size * sr)\n#     chunk_size = window_size * sr\n#     chunks = []\n#     for chunk_idx in range(chunks_count):\n#         left = chunk_idx * sr\n#         right = min(left + chunk_size, len(wav))\n#         chunks.append(wav[left:right])\n#     chunk_tensor = torch.tensor(chunks)\n#     return chunk_tensor\n\n# file_list = os.listdir(os.path.join(data_root, 'test_soundscapes'))\n\n# # This is where we will store our results\n# treshold = 0.5\n# pred = {'row_id': [], 'target': []}\n# model.eval()\n# with torch.no_grad():\n#     # Process audio files and make predictions\n#     for fname in file_list:\n#         prefix = fname.split('.')[0]\n#         # Complete file path\n#         fpath = os.path.join(data_root, 'test_soundscapes', fname)\n#         wav, sr = load_wav(fpath, 0, None)\n#         chunk_tensor = chunk_wav(wav, sr, window_size=5)\n\n#         # Open file with librosa and split signal into 5-second chunks\n#         # sig, rate = librosa.load(path)\n#         # ...\n\n#         # Let's assume we have a list of 12 audio chunks (1min / 5s == 12 segments)\n#         chunk_score = model(chunk_tensor.to(device))['logits'].cpu().numpy()\n\n#         # Make prediction for each chunk\n#         # Each scored bird gets a random value in our case\n#         # since we don't actually have a model\n#         for i, all_score in enumerate(chunk_score):        \n#             chunk_end_time = (i + 1) * 5\n#             for bird in test_birds:\n\n#                 # This is our random prediction score for this bird\n#                 bird_score = all_score[species2id[bird]]\n\n#                 # Assemble the row_id which we need to do for each scored bird\n#                 row_id = prefix + '_' + bird + '_' + str(chunk_end_time)\n\n#                 # Put the result into our prediction dict and\n#                 # apply a \"confidence\" threshold of 0.5\n#                 pred['row_id'].append(row_id)\n#                 pred['target'].append(True if bird_score > treshold else False)\n\n\n# # Make a new data frame and look at some results        \n# results = pd.DataFrame(pred, columns = ['row_id', 'target'])\n\n# # Quick sanity check\n# print(results.head()) \n    \n# # Convert our results to csv\n# results.to_csv(\"submission.csv\", index=False)    ","metadata":{"execution":{"iopub.status.busy":"2022-04-23T15:00:22.026005Z","iopub.execute_input":"2022-04-23T15:00:22.026332Z","iopub.status.idle":"2022-04-23T15:00:22.45859Z","shell.execute_reply.started":"2022-04-23T15:00:22.026291Z","shell.execute_reply":"2022-04-23T15:00:22.457601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test_fnames = [f for f in os.listdir(f'{data_root}/test_soundscapes') if f.endswith('.ogg')]\n# test_pd = []\n# for fname in test_fnames:\n#     fpath = os.path.join(data_root, 'test_soundscapes', fname)\n#     wav, sr = librosa.load(fpath, sr=None)\n#     prefix = fname.split('.')[0]\n#     window_size = 5 * sr\n#     for i, chunk in enumerate(wav[::window_size]):\n#         end_time = (i + 1) * 5\n#         samples = [{\n#             'row_id': f'{prefix}_{b}_{end_time}',\n#             'file_id': fname,\n#             'bird': b,\n#             'end_time': end_time\n#         } for b in test_birds]\n#         test_pd.extend(samples)\n        \n# test_pd = pd.DataFrame(test_pd)\n# test_pd","metadata":{},"execution_count":null,"outputs":[]}]}